{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the file\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "print(\"Reading the file\")\n",
    "\n",
    "\n",
    "df = pd.read_parquet('O:/DriveFiles/CCE/datasets/defectors/line_bug_prediction_splits/time/test.parquet.gzip')\n",
    "# print(df.head().T) # 'application/gzip'\n",
    "# Taking tanspose so the printing dataset will easy. \n",
    "\n",
    "# for i in range(0, 5):\n",
    "#     bytes_data = df['datetime'][i]\n",
    "#     string_data = bytes_data.decode('utf-8')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_top = df.head()\n",
    "\n",
    "# print(data_top.index)\n",
    "for row in data_top:\n",
    "    # print(row)\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "row_count = 0\n",
    "# iterating over indices\n",
    "# print(df.__dict__)\n",
    "\n",
    "# for col_names, col_contents in df.items():\n",
    "    # row_count += 1\n",
    "    # print(col_names)\n",
    "    # print(col_contents)\n",
    "\n",
    "# print the row count\n",
    "# print(row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'import logging\\nimport re\\nfrom typing import Dict, List, Optional\\n\\nfrom cachetools import TTLCache\\nfrom moto.core.exceptions import JsonRESTError\\n\\nfrom localstack.utils.aws import aws_stack\\nfrom localstack.utils.json import canonical_json\\nfrom localstack.utils.testutil import list_all_resources\\n\\nLOG = logging.getLogger(__name__)\\n\\n# cache schema definitions\\nSCHEMA_CACHE = TTLCache(maxsize=50, ttl=20)\\n\\n\\nclass ItemSet:\\n    \"\"\"Represents a set of items and provides utils to find individual items in the set\"\"\"\\n\\n    def __init__(self, items: List[Dict], key_schema: List[Dict]):\\n        self.items_list = items\\n        self.key_schema = key_schema\\n        self._build_dict()\\n\\n    def _build_dict(self):\\n        self.items_dict = {}\\n        for item in self.items_list:\\n            self.items_dict[self._hashable_key(item)] = item\\n\\n    def _hashable_key(self, item: Dict):\\n        keys = SchemaExtractor.extract_keys_for_schema(item=item, key_schema=self.key_schema)\\n        return canonical_json(keys)\\n\\n    def find_item(self, item: Dict) -> Optional[Dict]:\\n        key = self._hashable_key(item)\\n        return self.items_dict.get(key)\\n\\n\\nclass SchemaExtractor:\\n    @classmethod\\n    def extract_keys(cls, item: Dict, table_name: str) -> Optional[Dict]:\\n        key_schema = cls.get_key_schema(table_name)\\n        return cls.extract_keys_for_schema(item, key_schema)\\n\\n    @classmethod\\n    def extract_keys_for_schema(cls, item: Dict, key_schema: List[Dict]):\\n        result = {}\\n        for key in key_schema:\\n            attr_name = key[\"AttributeName\"]\\n            if attr_name not in item:\\n                raise JsonRESTError(\\n                    error_type=\"ValidationException\",\\n                    message=\"One of the required keys was not given a value\",\\n                )\\n            result[attr_name] = item[attr_name]\\n        return result\\n\\n    @classmethod\\n    def get_key_schema(cls, table_name: str) -> Optional[List[Dict]]:\\n        from localstack.services.dynamodb.provider import get_store\\n\\n        table_definitions: Dict = get_store().table_definitions\\n        table_def = table_definitions.get(table_name)\\n        if not table_def:\\n            raise Exception(f\"Unknown table: {table_name} not found in {table_definitions.keys()}\")\\n        return table_def[\"KeySchema\"]\\n\\n    @classmethod\\n    def get_table_schema(cls, table_name):\\n        key = f\"{aws_stack.get_region()}/{table_name}\"\\n        schema = SCHEMA_CACHE.get(key)\\n        if not schema:\\n            ddb_client = aws_stack.connect_to_service(\"dynamodb\")\\n            schema = ddb_client.describe_table(TableName=table_name)\\n            SCHEMA_CACHE[key] = schema\\n        return schema\\n\\n\\nclass ItemFinder:\\n    @staticmethod\\n    def find_existing_item(put_item: Dict, table_name=None) -> Optional[Dict]:\\n        from localstack.services.dynamodb.provider import ValidationException\\n\\n        table_name = table_name or put_item[\"TableName\"]\\n        ddb_client = aws_stack.connect_to_service(\"dynamodb\")\\n\\n        search_key = {}\\n        if \"Key\" in put_item:\\n            search_key = put_item[\"Key\"]\\n        else:\\n            schema = SchemaExtractor.get_table_schema(table_name)\\n            schemas = [schema[\"Table\"][\"KeySchema\"]]\\n            for index in schema[\"Table\"].get(\"GlobalSecondaryIndexes\", []):\\n                # TODO\\n                # schemas.append(index[\\'KeySchema\\'])\\n                pass\\n            for schema in schemas:\\n                for key in schema:\\n                    key_name = key[\"AttributeName\"]\\n                    key_value = put_item[\"Item\"].get(key_name)\\n                    if not key_value:\\n                        raise ValidationException(\\n                            \"The provided key element does not match the schema\"\\n                        )\\n                    search_key[key_name] = key_value\\n            if not search_key:\\n                return\\n\\n        req = {\"TableName\": table_name, \"Key\": search_key}\\n        existing_item = aws_stack.dynamodb_get_item_raw(req)\\n        if not existing_item:\\n            return existing_item\\n        if \"Item\" not in existing_item:\\n            if \"message\" in existing_item:\\n                table_names = ddb_client.list_tables()[\"TableNames\"]\\n                msg = (\\n                    \"Unable to get item from DynamoDB (existing tables: %s ...truncated if >100 tables): %s\"\\n                    % (\\n                        table_names,\\n                        existing_item[\"message\"],\\n                    )\\n                )\\n                LOG.warning(msg)\\n            return\\n        return existing_item.get(\"Item\")\\n\\n    @classmethod\\n    def list_existing_items_for_statement(cls, partiql_statement: str) -> List:\\n        table_name = extract_table_name_from_partiql_update(partiql_statement)\\n        if not table_name:\\n            return []\\n        all_items = cls.get_all_table_items(table_name)\\n        return all_items\\n\\n    @staticmethod\\n    def get_all_table_items(table_name: str) -> List:\\n        ddb_client = aws_stack.connect_to_service(\"dynamodb\")\\n        dynamodb_kwargs = {\"TableName\": table_name}\\n        all_items = list_all_resources(\\n            lambda kwargs: ddb_client.scan(**{**kwargs, **dynamodb_kwargs}),\\n            last_token_attr_name=\"LastEvaluatedKey\",\\n            next_token_attr_name=\"ExclusiveStartKey\",\\n            list_attr_name=\"Items\",\\n        )\\n        return all_items\\n\\n\\ndef extract_table_name_from_partiql_update(statement: str) -> Optional[str]:\\n    regex = r\"^\\\\s*(UPDATE|INSERT\\\\s+INTO|DELETE\\\\s+FROM)\\\\s+([^\\\\s]+).*\"\\n    match = re.match(regex, statement, flags=re.IGNORECASE | re.MULTILINE)\\n    return match and match.group(2)\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.iloc[0][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pythongithub'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythongithub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m repo \u001b[38;5;241m=\u001b[39m Repo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m repo\u001b[38;5;241m.\u001b[39mgit\u001b[38;5;241m.\u001b[39mrev_parse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pythongithub'"
     ]
    }
   ],
   "source": [
    "from git import Repo\n",
    "\n",
    "repo = Repo(\"lightning\")\n",
    "commit_hash = repo.git.rev_parse(\"HEAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104]\n",
      "import logging\n",
      "import re\n",
      "from typing import Dict, List, Optional\n",
      "\n",
      "from cachetools import TTLCache\n",
      "from moto.core.exceptions import JsonRESTError\n",
      "\n",
      "from localstack.utils.aws import aws_stack\n",
      "from localstack.utils.json import canonical_json\n",
      "from localstack.utils.testutil import list_all_resources\n",
      "\n",
      "LOG = logging.getLogger(__name__)\n",
      "\n",
      "# cache schema definitions\n",
      "SCHEMA_CACHE = TTLCache(maxsize=50, ttl=20)\n",
      "\n",
      "\n",
      "class ItemSet:\n",
      "    \"\"\"Represents a set of items and provides utils to find individual items in the set\"\"\"\n",
      "\n",
      "    def __init__(self, items: List[Dict], key_schema: List[Dict]):\n",
      "        self.items_list = items\n",
      "        self.key_schema = key_schema\n",
      "        self._build_dict()\n",
      "\n",
      "    def _build_dict(self):\n",
      "        self.items_dict = {}\n",
      "        for item in self.items_list:\n",
      "            self.items_dict[self._hashable_key(item)] = item\n",
      "\n",
      "    def _hashable_key(self, item: Dict):\n",
      "        keys = SchemaExtractor.extract_keys_for_schema(item=item, key_schema=self.key_schema)\n",
      "        return canonical_json(keys)\n",
      "\n",
      "    def find_item(self, item: Dict) -> Optional[Dict]:\n",
      "        key = self._hashable_key(item)\n",
      "        return self.items_dict.get(key)\n",
      "\n",
      "\n",
      "class SchemaExtractor:\n",
      "    @classmethod\n",
      "    def extract_keys(cls, item: Dict, table_name: str) -> Optional[Dict]:\n",
      "        key_schema = cls.get_key_schema(table_name)\n",
      "        return cls.extract_keys_for_schema(item, key_schema)\n",
      "\n",
      "    @classmethod\n",
      "    def extract_keys_for_schema(cls, item: Dict, key_schema: List[Dict]):\n",
      "        result = {}\n",
      "        for key in key_schema:\n",
      "            attr_name = key[\"AttributeName\"]\n",
      "            if attr_name not in item:\n",
      "                raise JsonRESTError(\n",
      "                    error_type=\"ValidationException\",\n",
      "                    message=\"One of the required keys was not given a value\",\n",
      "                )\n",
      "            result[attr_name] = item[attr_name]\n",
      "        return result\n",
      "\n",
      "    @classmethod\n",
      "    def get_key_schema(cls, table_name: str) -> Optional[List[Dict]]:\n",
      "        from localstack.services.dynamodb.provider import get_store\n",
      "\n",
      "        table_definitions: Dict = get_store().table_definitions\n",
      "        table_def = table_definitions.get(table_name)\n",
      "        if not table_def:\n",
      "            raise Exception(f\"Unknown table: {table_name} not found in {table_definitions.keys()}\")\n",
      "        return table_def[\"KeySchema\"]\n",
      "\n",
      "    @classmethod\n",
      "    def get_table_schema(cls, table_name):\n",
      "        key = f\"{aws_stack.get_region()}/{table_name}\"\n",
      "        schema = SCHEMA_CACHE.get(key)\n",
      "        if not schema:\n",
      "            ddb_client = aws_stack.connect_to_service(\"dynamodb\")\n",
      "            schema = ddb_client.describe_table(TableName=table_name)\n",
      "            SCHEMA_CACHE[key] = schema\n",
      "        return schema\n",
      "\n",
      "\n",
      "class ItemFinder:\n",
      "    @staticmethod\n",
      "    def find_existing_item(put_item: Dict, table_name=None) -> Optional[Dict]:\n",
      "        from localstack.services.dynamodb.provider import ValidationException\n",
      "\n",
      "        table_name = table_name or put_item[\"TableName\"]\n",
      "        ddb_client = aws_stack.connect_to_service(\"dynamodb\")\n",
      "\n",
      "        search_key = {}\n",
      "        if \"Key\" in put_item:\n",
      "            search_key = put_item[\"Key\"]\n",
      "        else:\n",
      "            schema = SchemaExtractor.get_table_schema(table_name)\n",
      "            schemas = [schema[\"Table\"][\"KeySchema\"]]\n",
      "            for index in schema[\"Table\"].get(\"GlobalSecondaryIndexes\", []):\n",
      "                # TODO\n",
      "                # schemas.append(index['KeySchema'])\n",
      "                pass\n",
      "            for schema in schemas:\n",
      "                for key in schema:\n",
      "                    key_name = key[\"AttributeName\"]\n",
      "                    key_value = put_item[\"Item\"].get(key_name)\n",
      "                    if not key_value:\n",
      "                        raise ValidationException(\n",
      "                            \"The provided key element does not match the schema\"\n",
      "                        )\n",
      "                    search_key[key_name] = key_value\n",
      "            if not search_key:\n",
      "                return\n",
      "\n",
      "        req = {\"TableName\": table_name, \"Key\": search_key}\n",
      "        existing_item = aws_stack.dynamodb_get_item_raw(req)\n",
      "        if not existing_item:\n",
      "            return existing_item\n",
      "        if \"Item\" not in existing_item:\n",
      "            if \"message\" in existing_item:\n",
      "                table_names = ddb_client.list_tables()[\"TableNames\"]\n",
      "                msg = (\n",
      "                    \"Unable to get item from DynamoDB (existing tables: %s ...truncated if >100 tables): %s\"\n",
      "                    % (\n",
      "                        table_names,\n",
      "                        existing_item[\"message\"],\n",
      "                    )\n",
      "                )\n",
      "                LOG.warning(msg)\n",
      "            return\n",
      "        return existing_item.get(\"Item\")\n",
      "\n",
      "    @classmethod\n",
      "    def list_existing_items_for_statement(cls, partiql_statement: str) -> List:\n",
      "        table_name = extract_table_name_from_partiql_update(partiql_statement)\n",
      "        if not table_name:\n",
      "            return []\n",
      "        all_items = cls.get_all_table_items(table_name)\n",
      "        return all_items\n",
      "\n",
      "    @staticmethod\n",
      "    def get_all_table_items(table_name: str) -> List:\n",
      "        ddb_client = aws_stack.connect_to_service(\"dynamodb\")\n",
      "        dynamodb_kwargs = {\"TableName\": table_name}\n",
      "        all_items = list_all_resources(\n",
      "            lambda kwargs: ddb_client.scan(**{**kwargs, **dynamodb_kwargs}),\n",
      "            last_token_attr_name=\"LastEvaluatedKey\",\n",
      "            next_token_attr_name=\"ExclusiveStartKey\",\n",
      "            list_attr_name=\"Items\",\n",
      "        )\n",
      "        return all_items\n",
      "\n",
      "\n",
      "def extract_table_name_from_partiql_update(statement: str) -> Optional[str]:\n",
      "    regex = r\"^\\s*(UPDATE|INSERT\\s+INTO|DELETE\\s+FROM)\\s+([^\\s]+).*\"\n",
      "    match = re.match(regex, statement, flags=re.IGNORECASE | re.MULTILINE)\n",
      "    return match and match.group(2)\n",
      "\n",
      "--------------------------------------------------\n",
      "                    search_key[key_name] = key_value\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for index, row in df.iterrows():\n",
    "    print(row['lines'])\n",
    "    bytes_data = row['content']\n",
    "    string_data = bytes_data.decode('utf-8')\n",
    "    print(string_data)\n",
    "    string_data = string_data.split(\"\\n\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(string_data[int(row['lines']) + 1])\n",
    "    i += 1\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\n  Update((integer:5, line 1:12 - 1:13), 3),\\n  Delete((identifier:y, line 2:8 - 2:9)),\\n  Delete((=:=, line 2:10 - 2:11)),\\n  Delete((integer:2, line 2:12 - 2:13)),\\n  Delete((assignment, line 2:8 - 2:13)),\\n  Delete((expression_statement, line 2:8 - 2:13))\\n]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import code_diff as cd\n",
    "\n",
    "\n",
    "output = cd.difference(\n",
    "    '''\n",
    "        x = 5\n",
    "        y = 2\n",
    "    ''',\n",
    "    '''\n",
    "        x = 3\n",
    "    ''',\n",
    "lang = \"python\")\n",
    "\n",
    "# Output: my_func -> say_helloworld\n",
    "\n",
    "str(output.edit_script())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_digits(num):\n",
    "    return num.replace(\"-\", \"\").replace(\"+\", \"\").replace('.','',1).replace(\"E\",\"\").isdigit()\n",
    "\n",
    "def check_floating(n1, n2):\n",
    "    if (not only_digits(n1)) or (not only_digits(n2)):\n",
    "        return False\n",
    "    #print(float(n1), float(n2))\n",
    "    if abs(float(n1)-float(n2))<1e-6:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_files(file1, file2):\n",
    "    try:\n",
    "        with open(file1) as f1, open(file2) as f2: \n",
    "            content1 = f1.read().strip().split()\n",
    "            content2 = f2.read().strip().split()\n",
    "            #print(content1)\n",
    "            #print(\"########\")\n",
    "            #print(content2)\n",
    "            if(len(content1) != len(content2)):\n",
    "                #print(\"length not same\")\n",
    "                #print(content1)\n",
    "                #print(\"#####\")\n",
    "                #print(content2)\n",
    "                return False\n",
    "            for l1, l2 in zip(content1, content2):\n",
    "                if l1.strip() != l2.strip(): \n",
    "                    num1s = l1.strip().split(\" \")\n",
    "                    num2s = l2.strip().split(\" \")\n",
    "                    if(len(num1s) == len(num2s)):\n",
    "                        for idx in range(len(num1s)):\n",
    "                            if not check_floating(num1s[idx],num2s[idx]):\n",
    "                                #print_error(l1, l2)\n",
    "                                return False\n",
    "                    else:\n",
    "                        #print_error(l1, l2)\n",
    "                        return False\n",
    "            \n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(\"exception = \", e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, subprocess\n",
    "from subprocess import Popen, PIPE, TimeoutExpired\n",
    "\n",
    "def run_python(code, test_case_folder, idx):\n",
    "    root_path = f'garbage/{idx}'\n",
    "    isExist = os.path.exists(root_path)\n",
    "    if not isExist:\n",
    "        os.makedirs(root_path)\n",
    "\n",
    "    with open(f'{root_path}/Main.py', 'w+', encoding='utf8') as fw:\n",
    "        fw.write(code)\n",
    "    in_files = glob(test_case_folder+\"/in/*\")\n",
    "    p1 = subprocess.run([\"python\",\"-m\", \"py_compile\", f\"{root_path}/Main.py\"], stderr=PIPE)\n",
    "    return_code = p1.returncode\n",
    "    python2 = False\n",
    "    if (return_code):\n",
    "        p1 = subprocess.run([\"python2\",\"-m\", \"py_compile\", f\"{root_path}/Main.py\"], stderr=PIPE)\n",
    "        return_code = p1.returncode\n",
    "        python2=True\n",
    "\n",
    "    if(return_code):\n",
    "        print(\"doesnt compile\", return_code)\n",
    "        return False, 0, len(in_files)\n",
    "\n",
    "    did_not_match = 0\n",
    "    for in_file in in_files:\n",
    "        stripped_TC = open(in_file).read().strip()\n",
    "        with open(f'{root_path}/stripped_TC.txt', 'w+') as f:\n",
    "            f.write(stripped_TC)\n",
    "        cmd = f\"python {root_path}/Main.py < {root_path}/stripped_TC.txt > {root_path}/cmd_out.txt\"\n",
    "        if (python2):\n",
    "            cmd = cmd.replace(\"python\", \"python2\")\n",
    "        p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=subprocess.DEVNULL, close_fds=True)\n",
    "\n",
    "        # for Time limit exceeded cases\n",
    "        try:\n",
    "            outs, errs = p.communicate(timeout=5)\n",
    "        except TimeoutExpired:\n",
    "            p.kill()\n",
    "        \n",
    "        out_file = in_file.replace(\"in\", \"out\", 1).replace(\".in\", \".out\", 1)\n",
    "\n",
    "        p2 = subprocess.Popen([\"cp\",out_file, f\"{root_path}/cmd_out_match.txt\"])\n",
    "        p2.wait()\n",
    "        if not compare_files(f'{root_path}/cmd_out.txt', f'{root_path}/cmd_out_match.txt'):\n",
    "            did_not_match+=1\n",
    "            #print(in_file)\n",
    "    \n",
    "    subprocess.run([\"rm\",\"-rf\",f\"{root_path}\"])\n",
    "    return True, len(in_files)-did_not_match,len(in_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_folder = \"O:/DriveFiles/CCE\\datasets/atcoder_test_cases_3/data/atcoder_test_cases\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
